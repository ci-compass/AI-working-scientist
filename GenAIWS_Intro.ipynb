{
 "cells": [
  {
   "cell_type": "raw",
   "id": "49a58314",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A guide to Generative AI for the working scientist\"\n",
    "date: 2025-10-29\n",
    "author:\n",
    "  - name: Charles F. Vardeman II\n",
    "    id: cfv\n",
    "    orcid: 0000-0002-1825-0097\n",
    "    email: cvardema@nd.edu\n",
    "    affiliation:\n",
    "      - name: Center for Research Computing, University of Notre Dame\n",
    "        city: Notre Dame\n",
    "        state: IN\n",
    "        url: https://crc.nd.edu\n",
    "      - name: CI-Compass\n",
    "        url: https://ci-compass.org\n",
    "keywords:\n",
    "  - Generative AI in Science\n",
    "  - Science Agents\n",
    "license: \"CC BY\"\n",
    "citation:\n",
    "  type: document\n",
    "  container-title: Zenodo\n",
    "  doi: 10.5281/zenodo.10815117\n",
    "links:\n",
    "  - text: \"Video Presentation\"\n",
    "    url: \"https://www.youtube.com/@cicompass\"\n",
    "  - text: \"GitHub Repository\"\n",
    "    url: \"https://github.com/ci-compass/AI-working-scientist/\"\n",
    "funding: \"This project is supported by the U.S. National Science Foundation Office of Advanced Cyberinfrastructure in the Directorate for Computer Information Science under Grant #2127548\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49926eb8",
   "metadata": {},
   "source": [
    "## GitHub CI-compass Organization\n",
    "- [GitHub Link: https://github.com/ci-compass/AI-working-scientist/tree/main](https://github.com/ci-compass/AI-working-scientist/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984505e3",
   "metadata": {},
   "source": [
    "# A guide to Generative AI for the Working Scientist\n",
    "\n",
    "> This video is meant to be a rough guide to some of the concepts and to help understand generative AI and preparation for the NSF CyberInfrastructure Center of Excellence [CI-Compass](https://ci-compass.org/) [Virtual Workshop - AI Meets CI: Intelligent Infrastructure for Major & Midscale Facilities](https://ci-compass.org/news-and-events/events/virtual-workshop-ai-meets-ci-intelligent-infrastructure-for-major-and-midscale-facilities/). The purpose is to start from the beginning and try to de-mystify **chatbot** based Generative AI.\n",
    "\n",
    "![](https://ci-compass.org/assets/629872/300x/ai_meets_ci_recreation_1.png)\n",
    "\n",
    "## Chat based generative AI\n",
    "\n",
    "### Chat interface\n",
    "- [OpenAI ChatGPT](https://chatgpt.com/)\n",
    "- [Anthropic Claude](https://claude.ai/new)\n",
    "- [Google Gemini](https://gemini.google.com/)\n",
    "\n",
    "### Terminal Based Coding Agents\n",
    "- [Anthropic Claude Code Cli](https://www.claude.com/product/claude-code)\n",
    "- [OpenAI Codex Cli](https://github.com/openai/codex)\n",
    "- [Google Gemini Cli](https://geminicli.com/)\n",
    "- [Microsoft Visual Studio Code](https://code.visualstudio.com/)\n",
    "\n",
    "### Notebook Based AI\n",
    "- [AnswerAI SolveIT](https://solve.it.com)\n",
    "- [Google Collab](https://developers.googleblog.com/en/data-science-agent-in-colab-with-gemini/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef43300",
   "metadata": {},
   "source": [
    "## Andrej Karpathy \"Deep Dive into LLMs like ChatGPT\"\n",
    "![](./images/karpathy-yt.png)\n",
    "\n",
    "- [Deep Dive into LLMs like ChatGPT: https://youtu.be/7xTGNNLPyMI](https://youtu.be/7xTGNNLPyMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1191d",
   "metadata": {},
   "source": [
    "## Andrej Karpathy - How I use LLMs\"\n",
    "![](./images/karpathy-yt-use.png)\n",
    "\n",
    "- [How I use LLMs YT link: https://youtu.be/EWvNQjAaOHw](https://youtu.be/EWvNQjAaOHw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf28ba6",
   "metadata": {},
   "source": [
    "## \"Foundation Models\"\n",
    "\n",
    "- [What is are Foundation Models? https://aws.amazon.com/what-is/foundation-models/](https://aws.amazon.com/what-is/foundation-models/)\n",
    "- Notion of \"Transfer Learning\" - That is you can start with a pretrained network for a new application or task.\n",
    "    - [J. Howard and S. Ruder, ‚ÄúUniversal Language Model Fine-tuning for Text Classification,‚Äù in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 328‚Äì339.https://aclanthology.org/P18-1031/](https://aclanthology.org/P18-1031/)\n",
    "\n",
    "<hr>\n",
    "\n",
    "- **Multimodal \"Text\" models:** GPT-5; Anthropic Claude Opus, Sonnet; Google Gemini 2.5; Meta LAMMA, Deepseek R1\n",
    "- **Image or Video Generation from text:** OpenAI Sora, [Google Imagen](https://imagen.research.google/)\n",
    "- **Science Foundation Models:** [Futurehouse ether0](https://www.futurehouse.org/research-announcements/ether0-a-scientific-reasoning-model-for-chemistry)\n",
    "    - [M. Hu et al., ‚ÄúA survey of scientific Large Language Models: From data foundations to agent frontiers,‚Äù arXiv [cs.CL], 28-Aug-2025. https://arxiv.org/abs/2508.21148](https://arxiv.org/abs/2508.21148)\n",
    "    - [Towards a Physics Foundation Model https://arxiv.org/html/2509.13805v2](https://arxiv.org/html/2509.13805v2)\n",
    "    - [H. Cui et al., ‚ÄúTowards multimodal foundation models in molecular cell biology,‚Äù Nature, vol. 640, no. 8059, pp. 623‚Äì633, Apr. 2025.https://www.nature.com/articles/s41586-025-08710-y](https://www.nature.com/articles/s41586-025-08710-y)\n",
    "    - [Nurips 2024: Foundation Models for Science: Progress, Opportunities, and Challenges](https://neurips.cc/virtual/2024/workshop/84714)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c88b76",
   "metadata": {},
   "source": [
    "## \"Autocomplete on Steroids\"\n",
    "- [Vaswami et al., \"Attention is all you need.\",https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src=\"https://arxiv.org/html/1706.03762v7/Figures/ModalNet-21.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5a18b",
   "metadata": {},
   "source": [
    "### \"Groking\" -- Memorization vs Generalization\n",
    "> \"Out of Distribution Phenomenon\" where  model initially overfits to the training set, and then suddenly generalizes to unseen test samples after continuous training\n",
    "\n",
    "![An interactive introduction to grokking and mechanistic interpretability](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/XpCnhaAQrssq8tJBG/rfpm8jhcd5kog1mqi8jn)\n",
    "\n",
    "- [An interactive introduction to grokking and mechanistic interpretability](https://www.alignmentforum.org/posts/XpCnhaAQrssq8tJBG/an-interactive-introduction-to-grokking-and-mechanistic)\n",
    "- [Do Machine Learning Models Memorize or Generalize? https://pair.withgoogle.com/explorables/grokking/](https://pair.withgoogle.com/explorables/grokking/)\n",
    "- [Anthropic Transformer Circuits](https://transformer-circuits.pub/)\n",
    "- [Y. Tian, ‚ÄúProvable scaling laws of feature emergence from learning dynamics of grokking,‚Äù arXiv [cs.LG], 30-Sept-2025.https://arxiv.org/html/2509.21519v3](https://arxiv.org/html/2509.21519v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298a2e9",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "What makes the transformer architecture more than \"Autocomplete on steroids?\" What don't we understand about the scale of the architecture.\n",
    "\n",
    "##### ü§ñReplyü§ñ<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great question! The transformer architecture is indeed often described as \"autocomplete on steroids,\" but there's something deeper happening that we're still trying to understand.\n",
    "\n",
    "The key mystery is **emergence** - capabilities that appear seemingly out of nowhere as models scale up. Small models do behave mostly like fancy autocomplete, but larger models start exhibiting behaviors they weren't explicitly trained for, like:\n",
    "\n",
    "- Reasoning through multi-step problems\n",
    "- Translating between languages they've barely seen\n",
    "- Understanding analogies and abstract concepts\n",
    "- Performing arithmetic (even though they're just predicting text!)\n",
    "\n",
    "This connects to that \"grokking\" phenomenon you saw in the notes - where a model suddenly generalizes after appearing to just memorize. But at the scale of models like GPT-4 or Claude, we see this happening across many different capabilities simultaneously.\n",
    "\n",
    "**What we don't understand:**\n",
    "\n",
    "1. **Why certain capabilities emerge at specific model sizes** - there seem to be \"phase transitions\" where new abilities suddenly appear\n",
    "2. **How the models actually represent knowledge internally** - the Anthropic \"Transformer Circuits\" work is trying to reverse-engineer this\n",
    "3. **Whether there are fundamental limits** to what emerges with scale, or if we'll keep seeing new capabilities\n",
    "\n",
    "Have you thought about what kinds of capabilities you'd want to investigate or understand better? Or are you curious about how researchers are trying to \"look inside\" these models?"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
