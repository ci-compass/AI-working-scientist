{
 "cells": [
  {
   "cell_type": "raw",
   "id": "49a58314",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A guide to Generative AI for the working scientist\"\n",
    "date: 2025-10-29\n",
    "author:\n",
    "  - name: Charles F. Vardeman II\n",
    "    id: cfv\n",
    "    orcid: 0000-0002-1825-0097\n",
    "    email: cvardema@nd.edu\n",
    "    affiliation:\n",
    "      - name: Center for Research Computing, University of Notre Dame\n",
    "        city: Notre Dame\n",
    "        state: IN\n",
    "        url: https://crc.nd.edu\n",
    "      - name: CI-Compass\n",
    "        url: https://ci-compass.org\n",
    "keywords:\n",
    "  - Generative AI in Science\n",
    "  - Science Agents\n",
    "license: \"CC BY\"\n",
    "citation:\n",
    "  type: document\n",
    "  container-title: Zenodo\n",
    "  doi: 10.5281/zenodo.10815117\n",
    "links:\n",
    "  - text: \"Video Presentation\"\n",
    "    url: \"https://www.youtube.com/@cicompass\"\n",
    "  - text: \"GitHub Repository\"\n",
    "    url: \"https://github.com/ci-compass/AI-working-scientist/\"\n",
    "funding: \"This project is supported by the U.S. National Science Foundation Office of Advanced Cyberinfrastructure in the Directorate for Computer Information Science under Grant #2127548\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49926eb8",
   "metadata": {},
   "source": [
    "## GitHub CI-compass Organization\n",
    "- [GitHub Link: https://github.com/ci-compass/AI-working-scientist/tree/main](https://github.com/ci-compass/AI-working-scientist/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984505e3",
   "metadata": {},
   "source": [
    "# A guide to Generative AI for the Working Scientist\n",
    "\n",
    "> This video is meant to be a rough guide to some of the concepts and to help understand generative AI and preparation for the NSF CyberInfrastructure Center of Excellence [CI-Compass](https://ci-compass.org/) [Virtual Workshop - AI Meets CI: Intelligent Infrastructure for Major & Midscale Facilities](https://ci-compass.org/news-and-events/events/virtual-workshop-ai-meets-ci-intelligent-infrastructure-for-major-and-midscale-facilities/). The purpose is to start from the beginning and try to de-mystify **chatbot** based Generative AI.\n",
    "\n",
    "![](https://ci-compass.org/assets/629872/300x/ai_meets_ci_recreation_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7756de",
   "metadata": {},
   "source": [
    "## \"Prompt Engineering\" vs \"Context Engineering\"\n",
    "\n",
    "> \"Context refers to the set of tokens included when sampling from a large-language model (LLM). The engineering problem at hand is optimizing the utility of those tokens against the inherent constraints of LLMs in order to consistently achieve a desired outcome. Effectively wrangling LLMs often requires thinking in context — in other words: considering the holistic state available to the LLM at any given time and what potential behaviors that state might yield.\"\n",
    "\n",
    "![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ffaa261102e46c7f090a2402a49000ffae18c5dd6-2292x1290.png&w=3840&q=75)\n",
    "---\n",
    "- [Anthropic Engineering, \"Effective context engineering for AI agents\", Sep 29, 2025, https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f9ec9",
   "metadata": {},
   "source": [
    "## \"Context Window -- Trained Model\"\n",
    "Think of an LLM as a kind of supercharged text predictor: you give it a sequence of tokens (words or subwords) and it predicts the next token, then the next, etc. What we call the context window is simply how many tokens the model can look back at when making each prediction.\n",
    "\n",
    "- If a model has a context window of, say, 4,096 tokens, then when it’s about to predict token N, it only “knows” about tokens N-4,095 through N-1 (plus whatever internal state) — it cannot directly “see” tokens older than that.\n",
    "\n",
    "- Everything the model uses to ground its prediction must be inside that window — the user prompt, the system instructions, examples, retrieved documents, conversation history, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be634c62",
   "metadata": {},
   "source": [
    "### Why does the Context Window Matter?\n",
    "1. **Scope of what the model knows in this invocation -- Stateless Model**\n",
    "> Because the model cannot remember everything ever said, only what fits into its window — if you want it to reference a piece of text, you must include it (or a compressed version of it) in the window.\n",
    "\n",
    "2. **Management of context = performance trade-offs**\n",
    "> The more tokens you feed (longer history, more retrieved docs, more examples), the richer the information the model has — but you are limited by the window size. If you exceed it, older tokens get truncated (lost). If you fill it with irrelevant stuff, you can confuse the model (context noise) rather than help it. Karpathy calls this “the delicate art and science of filling the context window with just the right information for the next step.”\n",
    "\n",
    "3. **Analogy: human coworker with short-term memory**\n",
    "> Karpathy uses an analogy: the LLM is like a coworker who has anterograde amnesia — they forget everything beyond a short timeframe. So if you want them to reference something older, you must remind them (i.e., re-include it in the window)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98a0c2",
   "metadata": {},
   "source": [
    "### Multimodal Context Window\n",
    ">Now imagine we extend that idea: Instead of feeding the model just text tokens, we also feed in image tokens, audio tokens, video frame tokens, sensor tokens, etc. \n",
    "\n",
    "**Each modality has its own tokenizer:**\n",
    "\n",
    "- Text → word/subword tokens\n",
    "\n",
    "- Images → small patch tokens (like 16×16 pixels each)\n",
    "\n",
    "- Audio → waveform chunks or spectrogram tokens\n",
    "\n",
    "All of those get projected into the same vector space and concatenated into one long sequence.\n",
    "That sequence is the multimodal context window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e336926",
   "metadata": {},
   "source": [
    "## Recall and large context windows\n",
    "-[N. F. Liu et al., “Lost in the middle: How language models use long contexts,” Trans. Assoc. Comput. Linguist., vol. 12, pp. 157–173, Feb. 2024.https://ar5iv.labs.arxiv.org/html/2307.03172](https://ar5iv.labs.arxiv.org/html/2307.03172)\n",
    "\n",
    "<hr>\n",
    "\n",
    "![](https://ar5iv.labs.arxiv.org/html/2307.03172/assets/x1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670dfc0",
   "metadata": {
    "time_run": "5:42:30p"
   },
   "outputs": [],
   "source": [
    "from dialoghelper import *\n",
    "fc_tool_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b328965",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Tools available from `fastcore.tools`:\n",
    "\n",
    "- &`rg`: Run the `rg` command with the args in `argstr` (no need to backslash escape)\n",
    "- &`sed`: Run the `sed` command with the args in `argstr` (e.g for reading a section of a file)\n",
    "- &`view`: View directory or file contents with optional line range and numbers\n",
    "- &`create`: Creates a new file with the given content at the specified path\n",
    "- &`insert`: Insert new_str at specified line number\n",
    "- &`str_replace`: Replace first occurrence of old_str with new_str in file\n",
    "- &`strs_replace`: Replace for each str pair in old_strs,new_strs\n",
    "- &`replace_lines`: Replace lines in file using start and end line-numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd1f3e",
   "metadata": {
    "time_run": "5:42:30p"
   },
   "outputs": [],
   "source": [
    "from fastcore.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfc81e",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Tools available from `fastcore.tools`:\n",
    "\n",
    "- &`rg`: Run the `rg` command with the args in `argstr` (no need to backslash escape)\n",
    "- &`sed`: Run the `sed` command with the args in `argstr` (e.g for reading a section of a file)\n",
    "- &`view`: View directory or file contents with optional line range and numbers\n",
    "- &`create`: Creates a new file with the given content at the specified path\n",
    "- &`insert`: Insert new_str at specified line number\n",
    "- &`str_replace`: Replace first occurrence of old_str with new_str in file\n",
    "- &`strs_replace`: Replace for each str pair in old_strs,new_strs\n",
    "- &`replace_lines`: Replace lines in file using start and end line-numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec67704",
   "metadata": {
    "time_run": "5:42:30p"
   },
   "outputs": [],
   "source": [
    "tool_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2dd6f",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Tools available from `dialoghelper`:\n",
    "\n",
    "- &`curr_dialog`: Get the current dialog info.\n",
    "- &`msg_idx`: Get absolute index of message in dialog.\n",
    "- &`add_html`: Send HTML to the browser to be swapped into the DOM using hx-swap-oob.\n",
    "- &`find_msg_id`: Get the current message id.\n",
    "- &`find_msgs`: Find messages in current specific dialog that contain the given information.\n",
    "  - (solveit can often get this id directly from its context, and will not need to use this if the required information is already available to it.)\n",
    "- &`read_msg`: Get the message indexed in the current dialog.\n",
    "  - To get the exact message use `n=0` and `relative=True` together with `msgid`.\n",
    "  - To get a relative message use `n` (relative position index).\n",
    "  - To get the nth message use `n` with `relative=False`, e.g `n=0` first message, `n=-1` last message.\n",
    "- &`del_msg`: Delete a message from the dialog.\n",
    "- &`add_msg`: Add/update a message to the queue to show after code execution completes.\n",
    "- &`update_msg`: Update an existing message.\n",
    "- &`url2note`: Read URL as markdown, and add a note below current message with the result\n",
    "- &`msg_insert_line`: Insert text at a specific location in a message.\n",
    "- &`msg_str_replace`: Find and replace text in a message.\n",
    "- &`msg_strs_replace`: Find and replace multiple strings in a message.\n",
    "- &`msg_replace_lines`: Replace a range of lines in a message with new content.\n",
    "  - Always first use `read_msg( msgid=msgid, n=0, relative=True, nums=True)` to view the content with line numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff4bef",
   "metadata": {},
   "source": [
    "### \"Thinking Models and Chain of Thought\" (Deepseek-R1 from Karpathy Video)\n",
    "![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgVgTjwA0IzKekrQoMziCmDXjO10QKjdDdzK1Oj8bZToPOI6VjVzTKXZ6vnWvAGOdVnWznJK2ZZjfBuTLojobayI_yrvlFzE3dCErF2j5wKLGFWAkuGP9-r-hMrqFivnjYhbCIu7HFINSmHu4wUjlKHfJxWHZ8Y7CYUowWvxTeRJhQEAUswGh2fUd3VHA/s2500/chainofthought.png)\n",
    "\n",
    "- [Language Models Perform Reasoning via Chain of Thought (May 2022)https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7145b",
   "metadata": {},
   "source": [
    "## ReACT\n",
    "![](https://react-lm.github.io/files/diagram.png)\n",
    "\n",
    "- [ReAct: Synergizing Reasoning and Acting in Language Models Blog: https://react-lm.github.io/](https://react-lm.github.io/)\n",
    "- [S. Yao et al., “ReAct: Synergizing reasoning and acting in language models,” Int Conf Learn Represent, vol. abs/2210.03629, Oct. 2022. https://openreview.net/forum?id=WE_vluYUL-X](https://openreview.net/forum?id=WE_vluYUL-X)"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
